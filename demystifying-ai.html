<!DOCTYPE html>
<html lang="en">

  <head>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Down the Rabbit Hole</title>

    <!-- Bootstrap core CSS -->
    <link href="/static/vendor/bootstrap/css/bootstrap.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="/static/vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="/static/css/clean-blog.css" rel="stylesheet">

	<!-- MathJax link -->
	<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>

  </head>

  <body>

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-light fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand" href="index">Down the Rabbit Hole</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav ml-auto">
            <li class="nav-item">
              <a class="nav-link" href="index">Home</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="blog">Blog</a>
            </li>
           <li class="nav-item">
              <a class="nav-link" href="contact_alt">Contact</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Page Header -->
    <header class="masthead" style="background-image: url('/static/img/demystifying-ai/dodo-1.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Demystifying AI</h1>
              <h2 class="subheading">Unpacking some of the terms and concepts surrounding the hype around AI</h2>
              <span class="meta">Written by
                <a href="index">Liam Peet-Pare</a>
                sometime in 2018</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <h2 class="section-heading">So many terms, but what do they mean?</h2>
      <p>This post was originally written as an attempt to explain AI and deep learning
        to managers at a previous job where I was working as a data scientist. There was
        an extraordinary amount of hype over "AI" and an even more extraordinary lack of
        understanding of what various terms mean or where they come from.<p>
          
			<p>Artificial Intelligence, machine learning, deep learning – these terms probably
			sound familiar to you, but what do they actually mean? In only a couple of years,
			words that used to be sole domain of computer science and math departments,
			or lunch rooms at tech companies, have found their way into the everyday lexicon
			of the average person. The catch is, while it is now common to hear these words on
			the news, or read them in the newspaper, most people don’t really know what they mean.
			There is a lot of confusion and uncertainty around what AI is, and what machine
			learning and deep learning have to do with it. This post will attempt to clear up
			some of that confusion, and demystify these terms by putting them on a more solid
			definitional footing, without getting too deep into the weeds.</p>

            <p>What we choose to refer to as
			<a href="https://en.wikipedia.org/wiki/Artificial_intelligence">artificial
			intelligence</a>
			is somewhat arbitrary. For example, very few people would consider a calculator correctly
			summing two numbers as
			artificial intelligence, but most people would say facial recognition technology is
			artificial intelligence. This distinction isn’t made because of inherent differences
			in the algorithms responsible for executing the summation and identifying a face, but
			rather because we have become accustomed to computers adding numbers. Facial
			recognition on the other hand feels like a much more novel accomplishment, and also
			most of us would consider recognizing a face to be somehow a more “human” task than
			performing arithmetic. </p>

			<a href="#">
              <img class="img-fluid" src="/static/img/demystifying-ai/turing-machine.jpg" alt="A Turing Machine." style="width:650px;height:450px;">
            </a>
            <span class="caption text-muted">An example of early AI: a Turing machine.</span>

            <p>A computer successfully adding two numbers together is clearly an example of a
			machine performing what in human beings is a cognitive task, but, for the most part,
			when people talk about artificial intelligence, they are not referring to this kind
			of phenomenon. While it varies depending on who you are talking to, I think it is
			fair to say that what is considered artificial intelligence tends to be
			non-deterministic processes, that up until recently, computers were not very good
			at, and that seem to closely resemble what we generally think of as human thinking
			processes. This is perhaps a bit of a bulky definition, but I think it provides a
			somewhat accurate reflection of the inexact and shifting contours of what is meant
			by artificial intelligence. </p>

            <p><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine learning</a>,
			on the other hand, is a well-defined term in the realm of
			computer science. Machine learning generally refers to algorithms that allow a
			computer to learn from seeing examples of data. Wikipedia defines machine learning
			as “a subset of artificial intelligence in the field of computer science that often
			uses statistical techniques to give computers the ability to "learn"
			(i.e., progressively improve performance on a specific task) with data, without
			being explicitly programmed.” The majority of what we consider to be artificial
			intelligence today is driven by machine learning. Artificial intelligence can also
			come in the form of rules or logic based algorithms (the addition of two numbers is
			in fact an example of this) that do not learn from data, but rather execute decisions
			based on following logical rules. Many full-scale AI solutions contain a component of
			logic based rules, but in the current AI revolution there has been a move towards
			allowing machines to learn on their own, and away from human designed rules or
			guidelines.</p>

            <p>Finally, we have <a href="https://en.wikipedia.org/wiki/Deep_learning">deep
			learning</a>. Deep learning is a subset of machine learning, and refers to a class
			of algorithms vaguely inspired by biological nervous systems and brains. The base
			algorithm of most deep learning models is the artificial neural network, the
			development of which was inspired by the biological neural networks found in
			our brains. Deep learning models attempt to learn multiple levels of representations
			of the data that generally correspond to a hierarchy of concepts. It is worth
			remembering that while machine learning algorithms are often described via metaphor
			and analogy, the only truly accurate descriptions of what these models do is a
			mathematical one as they function by performing mathematical operations on numerical
			representations of data. Deep learning models have been wildly successful in recent
			years, and are largely responsible or driving the recent boom in AI. When you hear
			about some incredible new breakthrough in AI, it’s probably a deep learning model
			that has achieved it.</p>

			<a href="#">
              <img class="img-fluid" src="/static/img/demystifying-ai/neural-network.jpg" alt="A simple neural network with one hidden layer.">
            </a>
            <span class="caption text-muted">A simple neural network with one hidden layer.</span>

            <h2 class="section-heading">Artificial Intelligence is not new</h2>

            <p>For many of us, the past couple of years are the first time we have regularly
			heard about artificial intelligence outside of science fiction and Hollywood movies,
			but, surprisingly, most of these ideas are not new. Modern computers have of course
			been around since the 1950s, and the first algorithm was actually written by
			<a href="https://en.wikipedia.org/wiki/Ada_Lovelace">Ada Lovelace</a>
			in the 1840s to calculate Bernoulli numbers. That’s right; she wrote an algorithm
			before a working computer even existed, not to mention she faced a few obstacles
			being a woman in 19th Century England – pretty badass I’d say. If that didn’t make
			her interesting enough, she also happened to be the only legitimate child of Lord
			Byron.</p>

            <p>Similarly, machine learning is not a new concept. The term machine learning was
			coined in 1959 by Arthur Samuel, but even this was just a formalization of concepts
			that had already been around for a while. Many people have actually had exposure to
			machine learning models, and perhaps implemented machine learning models themselves
			without realizing it. For instance, linear and logistic regression are examples of
			machine learning algorithms, although they are rarely referred to as such.</p>

			<a href="#">
              <img class="img-fluid" src="/static/img/demystifying-ai/ada-lovelace.jpg" alt="Ada Lovelace, looking glamorous." style="width:650px;height:450px;">
            </a>
            <span class="caption text-muted">Ada Lovelace, looking glamorous.</span>


            <p>Even deep learning is not new. Artificial neural networks have been around for
			decades, and while many of the core ideas and refinements that have enabled the
			recent success of deep learning are slightly more recent, they certainly have not
			arrived in the past two years. In fact, many are more than 10 or 20 years old.
			Interestingly, a handful of Canadian academics are some of the foremost pioneers in
			the realm of deep learning, and, largely because of their influence, Canada has a
			disproportionately large presence in the world of AI. American tech giants are
			populated with graduates of Canadian universities, helping to drive their AI research
			forward. Université de Montréal and Mcgill (<a href="https://mila.quebec/en/">MILA</a>
			in particular), the University of
			Toronto, and the University of Alberta have some of the strongest researchers in
			machine learning in the entire world. To name a few academics and industry leaders
			with strong Canadian connections:
			<a href="https://en.wikipedia.org/wiki/Yoshua_Bengio">Yoshua Bengio</a>,
			<a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton">Geoffrey Hinton</a>,
			<a href="https://en.wikipedia.org/wiki/Yann_LeCun">Yann LeCun</a>,
			<a href="https://en.wikipedia.org/wiki/Richard_S._Sutton">Richard
			Sutton</a>, <a href="http://www.cs.toronto.edu/~ilya/">Ilya Sutskever</a>,
			and <a href="https://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a>.
			By no means does Canada occupy some sort
			of special place in the world of machine learning, but as a Canadian I find it
			interesting to see that Canada is home to some of the academic leaders who helped
			spark the deep learning revolution. Furthermore, it goes without saying that many of
			the intellectual leaders in the realm of machine learning are not Canadian, and of
			course many of the students and professors at Canadian Universities come from all
			over the world.</p>

            <h2 class="section-heading">So why does it seem like AI is this brand new thing? </h2>

            <p>The short answer to this question is that a lot of these techniques have only
			recently started to be truly effective. This of course begs the question as to why
			they have only started working well in the past few years, and the answer to this is
			mainly improved computational power, and much much more data. In order to “learn”,
			deep learning algorithms must perform an enormous number of computations, and this
			number of computations increases with the size and complexity of the deep learning
			model in question. </p>



            <p>Machine learning models are implemented by writing code, and this code can be
			written so as to optimize computational processes to be as efficient as possible,
			but this optimization only takes you so far – eventually it’s up to the hardware to
			do the actual computations. By an interesting turn of fate, the video game industry
			actually laid the groundwork for this piece of the puzzle. For decades, enormous sums
			of money have been invested in improving graphics processing units
			(<a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a>s) in order to
			allow machines to render increasingly complex video game graphics. Interestingly, the
			math that is required to render video game graphics (namely, matrix multiplication)
			is the same math needed to train deep learning algorithms, and thanks to years of
			investment in creating hardware optimized to perform these computations, modern GPUs
			are extremely efficient at doing this math. While you are less likely to hear about
			them, chipmakers, and NVIDIA in particular, have played a huge role in driving the AI
			boom. The appetite for increased specialized computational power is accelerating
			advances in this field, with companies like Google, Facebook, and Apple all recently
			developing their own versions of GPUs in order to run their deep learning models and
			decrease reliance on their competitors.</p>

            <p>An interesting side note here is that GPUs are not only the essential piece of
			hardware for training deep learning models, but much of the mining of
			cryptocurrencies is also performed with GPUs – either NVIDIA GPUs, or, in some cases,
			specialized hardware known as application-specific integrated circuits (ASICs).
			In short, if you bought
			<a href="https://ca.finance.yahoo.com/quote/NVDA?p=NVDA&.tsrc=fin-srch-v1">NVIDIA</a>
			stock 5 years ago you probably feel pretty good about yourself right now.</p>

			<a href="#">
              <img class="img-fluid" src="/static/img/demystifying-ai/gpu.jpg" alt="An NVIDIA GTX 1080 GPU." style="width:650px;height:450px;">
            </a>
            <span class="caption text-muted">An NVIDIA GTX 1080 GPU. Not Sure why tech hardware
			companies feel the need to make their products look like they come from a superhero movie.</span>

			<p>The other big piece of the puzzle leading to the success of deep learning
			algorithms is the availability of data. One feature that sets deep learning models
			apart from other machine learning models is that they tend to keep getting better
			and better the more data they see, whereas the effectiveness of many traditional
			machine learning methods plateaus relatively quickly as the amount of data used to
			train them increases. Thanks to the digitization of vast quantities of the world’s
			information, and the unprecedented creation of new data thanks to internet based
			applications and companies, there is now vastly more data available for machine
			learning algorithms to be trained on than was the case 10 to 15 years ago.</p>

			<p>This trend is also certainly not going away. With the advent of the
			<a href="https://en.wikipedia.org/wiki/Internet_of_things">Internet of
			Things<a> (i.e. appliances, cars, and other items with devices embedded
			in them enabling
			them to connect to the internet and share data), and the ineluctable transition to
			digital, internet based platforms, the amount of data created on a daily basis will
			continue to soar to staggering volumes, creating an environment in which AI
			algorithms will continually improve and solve ever more complicated tasks.</p>

			<h2 class="section-heading">What, exactly, can AI do?</h2>

			<p>Along with the confusion over what is meant by terms like artificial intelligence,
			machine learning, and deep learning, is a lack of clarity on what state of the
			art machine learning algorithms can actually do, and what they are used for. On
			one end of the spectrum is the idea that we are on the verge of
			<a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">artificial
			general intelligence</a>, and AI is a hugely dangerous and destabilizing force that
			we will be unable to control – a veritable Pandora’s Box. On the other end of the
			spectrum are those that think AI is all hype, and that it is not being used for
			anything useful, nor has it truly had any successes. Unsurprisingly, neither of these
			two points of view provides a very accurate depiction of the current state of
			artificial intelligence and machine learning. </p>

			<p>Machine learning algorithms can be used on a wide variety of different data types,
			and have recently been very successful when applied to data types and tasks that have
			traditionally been outside of the scope of machines’ abilities to do much with. For
			example, deep learning algorithms have proved to be extremely successful at
			performing image recognition tasks, in fact, often far better than humans on specific
			tasks. For example, <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> is
			an online database with over 14 million
			annotated/labelled images spread across over 20 thousand categories. There is an
			annual competition to identify the correct categories of these images using machine
			learning techniques, and starting in 2012 deep learning models began shattering the
			previous best results. The competition involves only one thousand out of the 20
			thousand categories, but state of the art deep learning models now achieve
			staggeringly low error rates on this dataset.</p>

			<p>Machine learning algorithms can also be used to analyze natural language.
			Using machines to perform analysis on natural language is not necessarily a recent
			phenomenon, but in recent years the efficacy of machine learning algorithms in this
			area has increased markedly. For example, a machine learning algorithm can be trained
			to identify hate speech, or to extract the names of people and entities in a
			document. Deep learning has been very effective at improving capabilities at working
			with textual data, known as Natural Language Processing
			(<a href="https://en.wikipedia.org/wiki/Natural_language_processing">NLP</a>),
			but I think it is
			fair to say that its capabilities remain somewhat more limited in NLP compared to
			image processing.</p>

			<p>Machine learning can of course also be used on more familiar structured datasets,
			such as economic data. For example, machine learning algorithms can be, and are,
			employed to predict prices in financial markets based on past prices and other input
			data. Google has touted their use of deep learning to analyze data related to the
			cooling of their server farms to achieve an astounding 40% reduction on their data
			centre cooling
			<a href="https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre-cooling-bill-40/">bill</a>.</p>

			<p>The list of examples of real or potential applications of machine learning is
			infinite, and it is a fool’s errand to try to provide an exhaustive catalogue of use
			cases, but, that being said, I think it is helpful to provide some concrete examples
			with the aim of reifying the rather more abstract categories of application mentioned
			above.</p>

			<h5>Text Categorization:</h5>

			<p>There are many situations in which large organisations and governments receive
			large quantities of textual data, and must separate this text into different
			categories. For example, many organisations monitor articles written in the media
			in order to understand how the public feels about the organisation, or how certain
			actions or policies are perceived in the media. This task involves identifying
			articles that are relevant to the organisation from the vast quantity of print
			media produced everyday. Lists of keywords combined with Boolean logic (e.g. “smart
			phone” AND NOT “iPhone”) can help to identify the articles of interest, but this
			approach is quite limited in its efficacy. On the other hand, if you have historical
			examples of which articles are important and unimportant for the organization, you
			can train a machine learning algorithm to learn to recognize the important articles
			and identify them automatically. This type of task is not limited to identifying
			important news articles, but can be applied in any situation involving the
			categorization or classification of textual data.</p>

			<h5>Chat-bots:</h5>

			<p>Another very useful application of AI is the creation of chat-bots, or virtual
			assistants. Chat-bots are virtual question answering tools that can help customers
			find information and answers without having to interact with a human agent or execute
			tasks, such as making an online purchase. Chat-bots vary in their sophistication, but
			if they are able to provide relevant information to customers or clients it is easy
			to see how useful they can be.</p>

			<h5>Recommender Systems:</h5>

			<p>Recommender systems are used to recommend products, services, or really anything
			to clients or customers. For example, Amazon and Netflix both leverage recommender
			systems as integral pieces of their business. Amazon of course uses recommender
			systems to suggest products for customers to purchase, while Netflix uses them to
			suggest shows and movies to watch.</p>

			<p>Whether or not you know it, you are interacting with AI algorithms on a daily
			basis. When your phone suggests the next word in a sentence, or autocorrects a
			misspelled word, there are machine learning algorithms running in the background.
			When you use Netflix, Amazon, or Facebook there are myriad machine learning
			algorithms working to suggest interesting movies, products, or advertisements to you.
			In fact, most social media websites and applications are continuously using AI to
			determine how best to encourage users to spend more time on their site or
			application. Even the United States Postal Service and Canada Post use AI to read
			addresses and postal codes from envelopes.</p>

			<a href="#">
              <img class="img-fluid" src="/static/img/demystifying-ai/bengio-lecun-hinton-ng.jpg" alt="Bengio, Lecun, Hinton, and Ng." style="width:620px;height:450px;">
            </a>
            <span class="caption text-muted">The big boys of machine learning. Left to right: Yann LeCun, Geoffrey Hinton, Yoshua Bengio,
			and Andrew Ng (from Andrew Ng's Facebook page).</span>

			<p>As these examples demonstrate, artificial intelligence is already enjoying
			widespread use, and investments into AI are paying dividends on a daily basis.
			Despite this, however, the type of AI we have today does not bear even a remote
			semblance to the kind of artificial general intelligence portrayed in science
			fiction. Today’s AI algorithms have become very good at solving some narrowly
			defined, highly restricted problems, but they have nothing resembling any kind
			of general intelligence. Even Google’s remarkably impressive and frightening new
			Google Assistant from their
			<a href="https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html">
			Duplex</a> project is actually very limited in what it is
			capable of doing.</p>

			<p>The virtual assistant’s uncanny ability to mimic human speech patterns and carry
			on a relatively complex conversation gives the impression that it truly understands
			the conversation it is engaging in, and makes it appear that it possesses some kind
			of general intelligence (if you haven’t seen it in action, I recommend checking out
			this <a href="https://www.youtube.com/watch?v=bd1mEm2Fy08">video</a>
			or the blog hyperlinked above). The reality is, however, that while the
			Duplex project is an incredible achievement, combining a variety of state of the art
			algorithms and feeding them enormous amounts of data to learn from, the assistant is
			actually quite limited in the scope of what it is capable of doing. It can book
			appointments and make reservations, but it does not possess any ability to “think on
			its own” and certainly won’t be plotting the destruction of humanity any time soon.</p>

			<p>This article has avoided going into any kind of technical details as to how these
			AI and machine learning algorithms work, and I have intentionally left out many
			important details for understanding the current state of AI, but in order to keep
			the length reasonable, certain editorial decisions had to be made. To sum things up,
			artificial intelligence, machine learning, and deep learning are not necessarily the
			same thing, but whether or not they know it, most people are actually talking about
			deep learning, and almost certainly talking about machine learning, when they talk
			about AI; the current AI revolution has probably been driven more by advances in
			processing power and data availability than by algorithmic innovation; and while we
			may not be close to building artificial general intelligence yet, the state of the
			art in AI already has many extremely effective applications. </p>


          </div>
        </div>
      </div>
    </article>

    <hr>

    <!-- Footer -->
    <footer>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <ul class="list-inline text-center">
              <li class="list-inline-item">
                <a href="#">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-twitter fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="#">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-facebook-f fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
              <li class="list-inline-item">
                <a href="#">
                  <span class="fa-stack fa-lg">
                    <i class="fas fa-circle fa-stack-2x"></i>
                    <i class="fab fa-github fa-stack-1x fa-inverse"></i>
                  </span>
                </a>
              </li>
            </ul>
              <p class="copyright text-muted">Copyright &copy; lpeetpare.github.io 2020</p>
          </div>
        </div>
      </div>
    </footer>

    <!-- Bootstrap core JavaScript -->
    <script src="/static/vendor/jquery/jquery.min.js"></script>
    <script src="/static/vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Custom scripts for this template -->
    <script src="/static/js/clean-blog.min.js"></script>

  </body>

</html>
